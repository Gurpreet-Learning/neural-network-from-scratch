{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network \n",
    "    - This is a implementation from scratch of neural network with scalable network architecture\n",
    "    - Used Numpy for matrix functions\n",
    "    \n",
    "* Author: Jonver Oro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import models\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import sklearn.datasets\n",
    "from math import log\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import accuracy_score,log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = load_breast_cancer()\n",
    "# x = test_data.data\n",
    "# y = test_data.target\n",
    "\n",
    "# Generate data\n",
    "x, y = sklearn.datasets.make_moons(1000, noise=0.20)\n",
    "#sklearn.datasets.make_blobs(n_samples=1000, centers=2, random_state=2)\n",
    "X_train = x\n",
    "Y_train = y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create neural network layer object class\n",
    "class activations:\n",
    "    def relu(self,Z):\n",
    "        R = np.maximum(0, Z)\n",
    "        return R\n",
    "    \n",
    "    def sigmoid(self,Z):\n",
    "        S = 1 / (1 + np.exp(-Z))\n",
    "        return S\n",
    "    \n",
    "    def tanh(self,x):\n",
    "        t=(np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "        return t\n",
    "    \n",
    "    def d_tanh(self,t):\n",
    "        dt=1-t**2\n",
    "        return dt\n",
    "    \n",
    "    def d_sigmoid(self,Z):\n",
    "        dS = Z * (1 - Z)\n",
    "        return dS\n",
    "\n",
    "    def d_relu(self,z):\n",
    "        dZ= np.where(z <= 0, 0, 1)\n",
    "        return dZ\n",
    "    \n",
    "class cost_functions:\n",
    "    \n",
    "    def cross_entropy(self,AL,Y):\n",
    "        #compute cross entropy cost\n",
    "        m = Y.shape[0] # shape of layer\n",
    "        cross_entropy = np.multiply(np.log(AL),Y)+np.multiply(1-Y,np.log(1-AL)) #cross entropy formula\n",
    "        cross_entropy = - np.sum(cross_entropy)/m\n",
    "        cost = np.squeeze(cross_entropy)\n",
    "        return cost\n",
    "    def cost_prime(self,AL,Y):\n",
    "        cost = (Y-AL)**2\n",
    "        cost = np.sum(cost)\n",
    "        cost = np.squeeze(cost)\n",
    "        return cost\n",
    "    \n",
    "    def mse(self,AL,Y):\n",
    "        n = AL.shape[1]\n",
    "        cost = (1./(2*n)) * np.sum((Y - AL) ** 2)\n",
    "        return cost\n",
    "    \n",
    "    def sum_squared(self,AL,Y):\n",
    "        return (np.sum(AL-Y))**2\n",
    "\n",
    "class layer:\n",
    "    def __init__(self,w,b,activation=None,last=True):\n",
    "        self.W = w # weights\n",
    "        self.B = b # bias\n",
    "        self.acti = activation\n",
    "        self.last = last\n",
    "        \n",
    "    def forward(self,A):\n",
    "        Z = np.dot(A,self.W)+self.B\n",
    "        if self.acti == 'relu':\n",
    "            A = activations().relu(Z)\n",
    "        elif self.acti == 'sigmoid':\n",
    "            A = activations().sigmoid(Z)\n",
    "        elif self.acti == 'tanh':\n",
    "            A = activations().tanh(Z)\n",
    "            \n",
    "        return A,Z\n",
    "    \n",
    "    def activation_derivative(self,A):\n",
    "        if self.acti == 'relu':\n",
    "            dA = activations().d_relu(A)\n",
    "        elif self.acti == 'sigmoid':\n",
    "            dA = activations().d_sigmoid(A)\n",
    "        elif self.acti == 'tanh':\n",
    "            dA = activations().d_tanh(A)\n",
    "        return dA\n",
    "    \n",
    "    def backward(self,A,A_prev,d_prev):\n",
    "        \n",
    "        dA = self.activation_derivative(A)\n",
    "        if self.last == True:\n",
    "            dW = np.dot(A_prev.T, (dA * d_prev))\n",
    "            d = self.W.dot(dA.T * d_prev.T)\n",
    "        else:\n",
    "            dW = np.dot(A_prev.T, (d_prev.T * dA))\n",
    "            d = self.W.dot(dA.T * d_prev)\n",
    "        dB = np.sum(dW, axis=0)\n",
    "        \n",
    "        return dW,dB,d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Simple Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6233858741736069\n",
      "0.5697714738798265\n",
      "0.5141420096553502\n",
      "0.4584664637000829\n",
      "0.40132259475934867\n",
      "0.3648845423813976\n",
      "0.3395797718254811\n",
      "0.32078998877160597\n",
      "0.30753927731189923\n",
      "0.2984194077120843\n",
      "0.29234569478581734\n",
      "0.28816762683793046\n",
      "0.28533939179877393\n",
      "0.28332548771658456\n",
      "0.28166767072420396\n",
      "0.28047499003362736\n",
      "0.2795177084556354\n",
      "0.278719161062533\n",
      "0.27805060870254794\n",
      "0.2774725494760207\n",
      "0.27697495362796\n",
      "0.27645683998781045\n",
      "0.27599701202203997\n",
      "0.27560086353887153\n",
      "0.27520322322525653\n",
      "0.27482653910195803\n",
      "0.27444849920855835\n",
      "0.27410393823636925\n",
      "0.2738110149679837\n",
      "0.2735506741615293\n",
      "0.27331551106717017\n",
      "0.27309432131043393\n",
      "0.27289837838636316\n",
      "0.2724906849378627\n",
      "0.2722165438815799\n",
      "0.27206752014788776\n",
      "0.27189713414983896\n",
      "0.27175103750566115\n",
      "0.27169787050087973\n",
      "0.2716860842147601\n",
      "----Weights----\n",
      "[[-0.17762298  0.2842099   0.28757528  0.36749103]\n",
      " [ 1.6686854   0.53185792 -0.03241047 -0.79630933]]\n",
      "[[-0.79546709  1.05500503]\n",
      " [ 0.61749165  0.68744297]\n",
      " [ 1.66945326  0.66944532]\n",
      " [ 1.4920886   0.49131528]]\n",
      "[[ 2.29249552]\n",
      " [-0.95413509]]\n",
      "----Final Outputs----\n",
      "0-0.08308693932611687\n",
      "0-0.032545747227795865\n",
      "0-0.022320641364447884\n",
      "0-0.016507879210210952\n",
      "0-0.1748199051490935\n",
      "0-0.3145364661820942\n",
      "1-0.5172002469607323\n",
      "1-0.9381179166119481\n",
      "1-0.9369339620087679\n",
      "1-0.9948884647758326\n",
      "1-0.7807711953625698\n",
      "0-0.042215428940482594\n",
      "0-0.9737319561091708\n",
      "0-0.27495262885184396\n",
      "0-0.00836534427037076\n",
      "1-0.9412647245066295\n",
      "1-0.9934121130965502\n",
      "1-0.9745781877560069\n",
      "1-0.6741907228098579\n",
      "1-0.9292461325172374\n",
      "0-0.04510503515631915\n",
      "1-0.538413454434895\n",
      "0-0.06750808468214001\n",
      "0-0.12884033014941074\n",
      "1-0.8030360290134433\n",
      "1-0.9817004923613931\n",
      "1-0.967705612564633\n",
      "1-0.9635065916118073\n",
      "1-0.9633296699809636\n",
      "0-0.018041461353704002\n",
      "0-0.019203762924777296\n",
      "1-0.9054737391951223\n",
      "0-0.09543442597695917\n",
      "1-0.955322393691267\n",
      "0-0.7740305098064704\n",
      "0-0.023502140901121918\n",
      "0-0.060488472488011834\n",
      "1-0.94833198410183\n",
      "1-0.9926864282890946\n",
      "0-0.6126100306907296\n",
      "0-0.021451293646789136\n",
      "0-0.2338000721561452\n",
      "1-0.991987397582927\n",
      "0-0.019874346495314072\n",
      "1-0.8335499382141873\n",
      "1-0.38682314596822664\n",
      "1-0.9379403785516556\n",
      "0-0.06005626376427211\n",
      "1-0.9444629073372455\n",
      "1-0.9288158214925278\n",
      "0-0.008213158465979727\n",
      "1-0.6214579598532585\n",
      "1-0.9894759707203965\n",
      "1-0.9944640582325907\n",
      "0-0.02654290834192222\n",
      "1-0.9457028316709436\n",
      "0-0.1989022243738329\n",
      "0-0.20648977396603543\n",
      "0-0.43294231597812616\n",
      "1-0.9323979571767369\n",
      "1-0.9882069194000884\n",
      "0-0.2092074004852823\n",
      "0-0.025677923701247465\n",
      "0-0.4812353017583169\n",
      "1-0.4595240228906899\n",
      "0-0.16461088226408516\n",
      "0-0.07206174252600327\n",
      "1-0.9881745736266633\n",
      "0-0.011044335625415867\n",
      "0-0.06136999201255685\n",
      "1-0.8816898665955555\n",
      "0-0.03902603319075838\n",
      "1-0.9435913779026178\n",
      "0-0.007642922009729372\n",
      "0-0.21161499844826404\n",
      "1-0.9933265187945205\n",
      "0-0.4954936625365702\n",
      "0-0.11842302547122949\n",
      "1-0.9811087905510749\n",
      "1-0.7491934855640217\n",
      "0-0.2034166146859885\n",
      "0-0.014129172500788718\n",
      "0-0.34982214287933483\n",
      "0-0.4388993604685621\n",
      "1-0.5659795676871218\n",
      "0-0.05397103754593725\n",
      "1-0.9940500324857378\n",
      "0-0.02925218495745292\n",
      "1-0.9958505085980817\n",
      "0-0.03185865726982527\n",
      "1-0.9274807014641421\n",
      "0-0.2769160893752458\n",
      "0-0.028047683896297245\n",
      "1-0.9935333076005336\n",
      "1-0.600626275535846\n",
      "1-0.9238193166387169\n",
      "0-0.020812643884667328\n",
      "1-0.9847860431806585\n",
      "1-0.9810267233909316\n",
      "0-0.01851384757971398\n",
      "0-0.05269991633205823\n",
      "0-0.0312581514949166\n",
      "1-0.9899657799776447\n",
      "0-0.1990759908424235\n",
      "0-0.0485053509515344\n",
      "0-0.03150233337423006\n",
      "0-0.5486266154795401\n",
      "1-0.9165007649333398\n",
      "1-0.8777474768421987\n",
      "1-0.9899171283155104\n",
      "0-0.23285670750506143\n",
      "0-0.018624709746137207\n",
      "0-0.022280040504301043\n",
      "0-0.026562440056009287\n",
      "0-0.1142286144097353\n",
      "0-0.026779372285750235\n",
      "1-0.9410958356113995\n",
      "0-0.010511461021250426\n",
      "1-0.9824441241775879\n",
      "0-0.8668751438121948\n",
      "0-0.014501267472526768\n",
      "0-0.0624002053704118\n",
      "1-0.34035622288496525\n",
      "1-0.8842171429562564\n",
      "1-0.6462408385358285\n",
      "1-0.6763646200871396\n",
      "1-0.3279856123829549\n",
      "0-0.1641240683417552\n",
      "0-0.033310659274075075\n",
      "0-0.017956141386684474\n",
      "0-0.024048263599586322\n",
      "0-0.3521097227813456\n",
      "0-0.012763456616327357\n",
      "0-0.11607979372363664\n",
      "0-0.08403190481746893\n",
      "1-0.883567096702205\n",
      "1-0.7570807105579703\n",
      "1-0.9417315083149652\n",
      "0-0.5458543324490536\n",
      "1-0.9880308103395189\n",
      "1-0.5876257614081831\n",
      "1-0.9970619587939775\n",
      "0-0.1386059932365462\n",
      "1-0.320907336456178\n",
      "0-0.20317467373807083\n",
      "0-0.48036386296337147\n",
      "1-0.7805061288056908\n",
      "0-0.12227874843075026\n",
      "1-0.9955256639877298\n",
      "0-0.04058927022421914\n",
      "0-0.628701347537772\n",
      "0-0.19667839379025664\n",
      "1-0.5189822439908924\n",
      "0-0.09796323304006488\n",
      "0-0.7034598031206512\n",
      "1-0.5611431990533963\n",
      "0-0.09235872754501433\n",
      "1-0.9911691332301336\n",
      "0-0.10119915531584978\n",
      "0-0.5039369985004997\n",
      "0-0.01318165237573693\n",
      "1-0.8424793026495642\n",
      "1-0.9562632919705814\n",
      "0-0.21777967808981266\n",
      "1-0.17304685653690835\n",
      "1-0.9898384298656481\n",
      "0-0.03882646434014808\n",
      "1-0.9645003932413793\n",
      "0-0.014640982152005185\n",
      "1-0.9696441631295885\n",
      "0-0.03806091276315322\n",
      "0-0.5794177845355942\n",
      "0-0.885456767885268\n",
      "1-0.6054150482935113\n",
      "1-0.9855426004151632\n",
      "0-0.01552068689523571\n",
      "1-0.9897343938986745\n",
      "0-0.07676843543837572\n",
      "1-0.9738777100431848\n",
      "1-0.9555099189485889\n",
      "0-0.4472181703429915\n",
      "1-0.6720971155979949\n",
      "0-0.01544220023111611\n",
      "0-0.03707894476502143\n",
      "1-0.9453048737117518\n",
      "1-0.9742381590934976\n",
      "1-0.9946595040597154\n",
      "1-0.9611557169325421\n",
      "1-0.750278817768239\n",
      "1-0.6147763610691452\n",
      "0-0.7620121530526976\n",
      "1-0.976724948830433\n",
      "0-0.028168083886434107\n",
      "1-0.8443410393382432\n",
      "0-0.249682077862987\n",
      "0-0.03156978202580709\n",
      "0-0.02306416683734752\n",
      "1-0.9864161720733226\n",
      "0-0.42919489157417046\n",
      "1-0.8829537351746879\n"
     ]
    }
   ],
   "source": [
    "#basic neural network with 3 layers\n",
    "epochs = 4000\n",
    "epsilon = 0.0001\n",
    "#dimensions\n",
    "l1_dim = x.shape[1]\n",
    "l2_dim = 4\n",
    "l3_dim = 2\n",
    "output_dim = 1\n",
    "\n",
    "#create weights and biases\n",
    "w1 = np.random.rand(l1_dim,l2_dim)\n",
    "b1 = np.random.rand(l2_dim)\n",
    "w2 = np.random.rand(l2_dim,l3_dim)\n",
    "b2 = np.random.rand(l3_dim)\n",
    "w3 = np.random.rand(l3_dim,output_dim)\n",
    "b3 = np.random.rand(output_dim)\n",
    "\n",
    "for e in range(0,epochs):\n",
    "    #FORWARD PASS\n",
    "    '''\n",
    "        Formula for forward pass\n",
    "        \n",
    "        let: σ(x) = activation function, A = activation,x = inputs, \n",
    "                    w = weights, b = bias\n",
    "                    \n",
    "            A = σ(w * x + b)\n",
    "    '''\n",
    "    A1,Z1 = layer(w1,b1,activation='relu').forward(x) # activation values\n",
    "    A2,Z2 = layer(w2,b2,activation='relu').forward(A1) # activation values\n",
    "    A3,Z3 = layer(w3,b3,activation='sigmoid').forward(A2) # activation values\n",
    "\n",
    "    #BACK PROPAGATION\n",
    "    '''\n",
    "        Formula for Back Propagation\n",
    "        \n",
    "        let: L = layer, l = index of current layer, ŷ= predicted vals,  δ = delta \n",
    "             δA = derivative of activation function, c = cost,ΔW = weights derivative,\n",
    "             ΔG = gradients derivative\n",
    "        \n",
    "        For 1st layer:\n",
    "            c = (y - ŷ) * 2\n",
    "            ΔW[l] = A[l-1] * c * δA[l]\n",
    "            δ[l] = w[l] * δA[l] * c\n",
    "            \n",
    "        For next layers:\n",
    "            ΔW[l] = A[l-1] * δ[l+1] * δA[l]\n",
    "            δ[l] = w[l] * δA[l] * δ[l+1]\n",
    "        \n",
    "        Bias Derivative Formula\n",
    "            δB = ∑ΔW[l]\n",
    "            \n",
    "        Gradient decent for updating gradients:\n",
    "            ΔG := - lr * ∇g\n",
    "            \n",
    "    '''\n",
    "    y = y.reshape(A3.shape)\n",
    "    cost = (y-A3)*2\n",
    "    dA3 = activations().d_sigmoid(A3)\n",
    "    dW3 = np.dot(A2.T, (dA3 * cost))\n",
    "    d3 = w3.dot(dA3.T * cost.T)\n",
    "\n",
    "    dA2 = activations().d_relu(A2)\n",
    "    dW2 = np.dot(A1.T, (d3.T * dA2))\n",
    "    d2 = w2.dot(dA2.T*d3)\n",
    "    \n",
    "    dA1 = activations().d_relu(A1)\n",
    "    dW1 = np.dot(x.T, (d2.T * dA1))\n",
    "    d1 = w1.dot(dA1.T*d2)\n",
    "  \n",
    "    dB3 = np.sum(dW3, axis=0) \n",
    "    dB2 = np.sum(dW2, axis=0) \n",
    "    dB1 = np.sum(dW1, axis=0) \n",
    "    \n",
    "\n",
    "    #WEIGHTS AND BIASES UPDATE\n",
    "    w3 += epsilon * dW3\n",
    "    b3 += epsilon * dB3\n",
    "    w2 += epsilon * dW2\n",
    "    b2 += epsilon * dB2\n",
    "    w1 += epsilon * dW1\n",
    "    b1 += epsilon * dB1\n",
    "\n",
    "    if (e+1) % 100 == 0:\n",
    "        y_pred = A3.T\n",
    "        y_pred = y_pred[0]\n",
    "        score = log_loss(y,y_pred)\n",
    "        print(score)\n",
    "        #print(dB1)\n",
    "        \n",
    "    if e == epochs-1:\n",
    "        print('----Weights----')\n",
    "        print(w1)\n",
    "        print(w2)\n",
    "        print(w3)\n",
    "\n",
    "        print('----Final Outputs----')\n",
    "        for i in range(0,int(len(y_pred)/5)):\n",
    "            print(f'{y.T[0][i]}-{y_pred[i]}')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Neural Network with scable architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating weights and biases\n",
      "Epoch: 100 Loss: 0.697551459977479\n",
      "Epoch: 200 Loss: 0.6652536726240843\n",
      "Epoch: 300 Loss: 0.6451007871914587\n",
      "Epoch: 400 Loss: 0.6165807009378045\n",
      "Epoch: 500 Loss: 0.5654787384522411\n",
      "Epoch: 600 Loss: 0.47219131723618696\n",
      "Epoch: 700 Loss: 0.4064921694360165\n",
      "Epoch: 800 Loss: 0.37707905929238267\n",
      "Epoch: 900 Loss: 0.3623741169877632\n",
      "Epoch: 1000 Loss: 0.3533799424577017\n",
      "Epoch: 1100 Loss: 0.3463241880733343\n",
      "Epoch: 1200 Loss: 0.340462951935137\n",
      "Epoch: 1300 Loss: 0.3362397197470868\n",
      "Epoch: 1400 Loss: 0.33221560987072796\n",
      "Epoch: 1500 Loss: 0.3272388899427922\n",
      "Epoch: 1600 Loss: 0.3191706329323338\n",
      "Epoch: 1700 Loss: 0.2964148948779128\n",
      "Epoch: 1800 Loss: 0.28513659374418476\n",
      "Epoch: 1900 Loss: 0.27135996390625655\n",
      "Epoch: 2000 Loss: 0.25886394721125955\n",
      "Epoch: 2100 Loss: 0.2469965874069153\n",
      "Epoch: 2200 Loss: 0.2355583579287141\n",
      "Epoch: 2300 Loss: 0.22505399419257593\n",
      "Epoch: 2400 Loss: 0.21671852650768236\n",
      "Epoch: 2500 Loss: 0.20998734500596675\n",
      "Epoch: 2600 Loss: 0.20477071157847124\n",
      "Epoch: 2700 Loss: 0.20104770550858103\n",
      "Epoch: 2800 Loss: 0.1982895129529618\n",
      "Epoch: 2900 Loss: 0.19635818981608447\n",
      "Epoch: 3000 Loss: 0.19535722513730536\n",
      "Epoch: 3100 Loss: 0.19634121294478013\n",
      "Epoch: 3200 Loss: 0.1976171909742974\n",
      "Epoch: 3300 Loss: 0.19911924028000197\n",
      "Epoch: 3400 Loss: 0.20080564843755325\n",
      "Epoch: 3500 Loss: 0.2022948740835394\n",
      "Epoch: 3600 Loss: 0.20330129850369433\n",
      "Epoch: 3700 Loss: 0.20456225892346475\n",
      "Epoch: 3800 Loss: 0.2056260662061335\n",
      "Epoch: 3900 Loss: 0.20561694783684065\n",
      "Epoch: 4000 Loss: 0.19800313997095395\n",
      "Epoch: 4100 Loss: 0.19448943893916584\n",
      "Epoch: 4200 Loss: 0.19235427363546306\n",
      "Epoch: 4300 Loss: 0.19046216658731013\n",
      "Epoch: 4400 Loss: 0.1885538676534999\n",
      "Epoch: 4500 Loss: 0.18670992523838192\n",
      "Epoch: 4600 Loss: 0.18482686834080148\n",
      "Epoch: 4700 Loss: 0.1829519973350881\n",
      "Epoch: 4800 Loss: 0.18125902055307802\n",
      "Epoch: 4900 Loss: 0.1796953182033825\n",
      "Epoch: 5000 Loss: 0.1783457321400225\n",
      "Epoch: 5100 Loss: 0.17715664240353907\n",
      "Epoch: 5200 Loss: 0.17570795897035696\n",
      "Epoch: 5300 Loss: 0.1743601158661382\n",
      "Epoch: 5400 Loss: 0.17329746550961334\n",
      "Epoch: 5500 Loss: 0.17231069377298425\n",
      "Epoch: 5600 Loss: 0.17130690612813007\n",
      "Epoch: 5700 Loss: 0.17037026084359513\n",
      "Epoch: 5800 Loss: 0.1694163929013362\n",
      "Epoch: 5900 Loss: 0.16857672650652775\n",
      "Epoch: 6000 Loss: 0.1679050560608557\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 6000\n",
    "learn_rate = 0.0001\n",
    "#define neural network architecture\n",
    "nn_layers = [\n",
    "    {'l_shape':x.shape[1],'activation':'relu'},\n",
    "    {'l_shape':6,'activation':'relu'},\n",
    "    {'l_shape':4,'activation':'relu'},\n",
    "    {'l_shape':2,'activation':'relu'},\n",
    "    {'l_shape':1,'activation':'sigmoid'}\n",
    "]\n",
    "cost_l = []\n",
    "L = len(nn_layers)\n",
    "weights = [0]*(len(nn_layers)-1)\n",
    "biases = [0]*(len(nn_layers)-1)\n",
    "activs = [0]*(len(nn_layers)-1)\n",
    "deltas = [0]*(len(nn_layers)-1)\n",
    "dActivs = [0]*(len(nn_layers)-1)\n",
    "#train neural network\n",
    "for e in range(0,epochs):\n",
    "    if e == 0:\n",
    "        print('creating weights and biases')\n",
    "        for l in range(0,L-1):\n",
    "            w = np.random.rand(nn_layers[l]['l_shape'],nn_layers[l+1]['l_shape']) #weight shape LixLi+1\n",
    "            weights[l] = w\n",
    "            b = np.random.rand(nn_layers[l+1]['l_shape'])#bias shape Li+1\n",
    "            biases[l]= b\n",
    "    \n",
    "    #forward propagation\n",
    "    for l in range(1,L):\n",
    "        if l == 1:\n",
    "            aL,aZ = layer(weights[l-1],biases[l-1],activation=nn_layers[l]['activation']).forward(X_train) #forward activation function\n",
    "            activs[l-1] = aL\n",
    "        else:\n",
    "            aL,aZ = layer(weights[l-1],biases[l-1],activation=nn_layers[l]['activation']).forward(activs[l-2]) #forward activation function\n",
    "            activs[l-1] = aL\n",
    "            \n",
    "            \n",
    "    #backward propagation\n",
    "    for l in range(L-2,-1,-1):\n",
    "        if l == L-2:\n",
    "            last_layer = True\n",
    "        else:\n",
    "            last_layer = False\n",
    "        layer_cls = layer(weights[l],biases[l],activation=nn_layers[l+1]['activation'],last=last_layer)\n",
    "        if l == L-2:\n",
    "            AL = activs[l]\n",
    "            Y_train = Y_train.reshape(AL.shape)\n",
    "            cost = (Y_train-AL)*2\n",
    "            d_prev = cost\n",
    "            dA = layer_cls.activation_derivative(activs[l])\n",
    "        else:\n",
    "            dA = layer_cls.activation_derivative(activs[l])\n",
    "            d_prev = deltas[l+1]\n",
    "            \n",
    "        if l != 0:  \n",
    "            dW,dB,d = layer_cls.backward(dA,activs[l-1],d_prev) #backward prop function\n",
    "        else:\n",
    "            dW,dB,d = layer_cls.backward(dA,X_train,d_prev) #backward prop function\n",
    "        deltas[l] = d       \n",
    "        weights[l] += learn_rate * dW\n",
    "        biases[l] += learn_rate * dB\n",
    "        \n",
    "    if (e+1) % 100 == 0:\n",
    "        y_pred = activs[L-2].T\n",
    "        y_pred = y_pred[0]\n",
    "        score = log_loss(y,y_pred)\n",
    "        cost_l.append(score)\n",
    "        print(f'Epoch: {e+1} Loss: {score}')\n",
    "\n",
    "plt.plot(cost_l)\n",
    "plt.ylabel('Model Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
